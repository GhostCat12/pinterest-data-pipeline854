{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up a  new conda environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n pinterest_conda_env\n",
    "conda activate pinterest_conda_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any libraries required to run the user_posting_emulation.py using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install <library_name>\n",
    "conda install -c conda-forge <library_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data infrastructure simulating Pinterest.\n",
    "\n",
    "The user_posting_emulation.py contains the login credentials for a RDS database, which contains three tables with data resembling data received by the Pinterest API when a POST request is made by a user uploading data to Pinterest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch processing : Configuring the EC2 Kafka client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSK cluster already created. To create an MSK cluster:\n",
    "\n",
    "Amazon MSK > Create cluster > Choose quick create > name cluster 'pinterest-msk-cluster > Choose Cluster type: provisioned > Apache Kafka version: 2.8.1 > Choose Broker type: Kafka.m5.large > Amazon EBS storage per broker: 100 GiB > Click create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The EC2 instance   \n",
    "AWS EC2: Cloud-hosted virtual machine\n",
    "\n",
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/ec2_instance.png)\n",
    "\n",
    "- Name: 0a60b9a8a831  \n",
    "- Instance ID : i-034925981e7bb03f3   \n",
    "- Instance type: t2.micro \n",
    "- Amazon Machine Image : Amazon linux 2023 AMI\n",
    "- Availability Zone: us-east-1a   \n",
    "- Public IPV4 DNS : ec2-34-207-200-90.compute-1.amazonaws.com \n",
    "- Public IPv4 address : 34.207.200.90 \n",
    "- Key pair name : 0a60b9a8a831-key-pair\n",
    "- Key pair type : RSA\n",
    "- Private key file format : .pem\n",
    "\n",
    "An IAM role was setup for the EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create .pem file locally\n",
    "EC2 > Instances > Search \"i-034925981e7bb03f3\" > Details > Key pair assigned at launch.  \n",
    "Copy the key pair and save inside a .pem file, and ensure that the .pem file has read-only permission for User class.   \n",
    ">chmod 400 0a60b9a8a831-key-pair.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Connect to the EC2 instance\n",
    "Follow the instructions on AWS on how to connect to EC2 through the CLI.  \n",
    "    \n",
    "> ssh -i \"0a60b9a8a831-key-pair.pem\" ec2-user@ec2-34-207-200-90.compute-1.amazonaws.com     \n",
    "    \n",
    "`ssh -i \"0a60b9a8a831-key-pair.pem\" ec2-user@ec2-34-207-200-90.compute-1.amazonaws.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Installing Kafka on the EC2 client \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been provided with access to an IAM authenticated MSK cluster.\n",
    "\n",
    "However MSK cluster can be setup manually by: \n",
    "\n",
    "- Creating the MSK cluster: \n",
    "    - MSK console > Create Cluster > Quick create    \n",
    "    - Cluster name :pinterest-msk-cluster    \n",
    "    - Cluster type : provisioned \n",
    "    - Apache Kafka version : 2.8.1   \n",
    "    - Broker type : kafka.m5.large   \n",
    "    - Amazon EBS storage per broker : 100 GiB    \n",
    "Create cluster  \n",
    "\n",
    "EC2 instance has been created, however to ensure the client machine can send data to the MSK cluster. \n",
    "VPC > Security > Security groups >  \n",
    "- Select the default security group associated with the cluster VPC (MSK console > properties tab > Networking settings section > Security groups applied) \n",
    "\n",
    "Edit inbound rules > Add rule. \n",
    "- Type column: All traffic     \n",
    "- Source column: ID of the security group of the client machine (found in EC2 console > security tab). \n",
    "- Save rules and cluster will accept all traffic from the client machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1 : First install java and check its version `java -version`. Then download and 'unzip' kafka version 2.12-2.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo yum install java-1.8.0\n",
    "\n",
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz   # Get the version of kafka to install\n",
    "tar -xzf kafka_2.12-2.8.1.tgz                                           # To 'unzip' or 'untar' the file \n",
    "rm kafka_2.12-2.8.1.tgz                                                 # Remove the compressed file, keeping only uncompressed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 : Installing IAM MSK authentication package on client EC2 machine.\n",
    "Enter inside the Kafka installation directory and then in the libs subdirectory. Download the IAM MSK authentication package from Github (Required for a connection to MSK clusters which uses IAM authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kafka_2.12-2.8.1/libs\n",
    "\n",
    "wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.5/aws-msk-iam-auth-1.1.5-all.jar  # Download msk iam authentication file\n",
    "export CLASSPATH=/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar # Set class path environment variable for MSK authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add above classpath environment variable to bashrc file to Configure the client to be able to use the IAM package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano /home/ec2-user/.bashrc\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/nano_bashrc_20231211.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should return this class path '/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar' if configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $CLASSPATH  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Step 3 : To assume the \"0a60b9a8a831-ec2-access-role\" IAM role, which contains the necessary permissions to authenticate the MSK cluster\n",
    "    \n",
    "IAM console > Roles > 0a60b9a8a831-ec2-access-role      \n",
    "- copy ARN      \n",
    "\n",
    "Trust relationships tab > Edit trust policy > Add a principal       \n",
    "- Selected IAM roles as the Principal type , Replace ARN with the 0a60b9a8a831-ec2-access-role ARN copied from ec2-access-role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 : Configure Kafka client to use AWS IAM authentication to the cluster \n",
    "In the EC2 client, inside the kafka/bin directory `cd kafka_2.12-2.8.1/bin`, modify the client.properties file `nano client.properties` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/aws/aws-msk-iam-auth instructions on configuring a Kafka client to use AWS IAM with AWS_MSK_IAM mechanism\n",
    "\n",
    "# Sets up TLS (Transport Layer Security) for encryption (cryptographic protocol) and SASL (Simple Authentication and Security Layer) for authN (framework for authentication and data security in Internet protocols).\n",
    "security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Creating Kafka topics\n",
    "\n",
    "Using the MSK Management Console to get cluster information\n",
    "Amazon MSK > pinterest-msk-cluster > View client information and save:  \n",
    "- Bootstrap servers Private endpoint (single-VPC)\n",
    "- Plaintext Apache Zookeeper connection string  \n",
    "\n",
    "topic names: \n",
    "- 0a60b9a8a831.pin for the Pinterest posts data\n",
    "- 0a60b9a8a831.geo for the post geolocation data\n",
    "- 0a60b9a8a831.user for the post user data\n",
    "\n",
    "In the EC2 client, enter inside the kafka folder, then inside the bin folder `cd kafka_2.12-2.8.1/bin`, to run the commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.pin\n",
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.geo\n",
    "./kafka-topics.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch processing : Connecting the MSK cluster to an S3 bucket.    \n",
    "The s3 bucket our data will be saved in is user-0a60b9a8a831-bucket (IAM role already set up to write to s3 bucket )\n",
    "\n",
    "Firstly, download confluent.io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i \n",
    "\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3 \n",
    "\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip \n",
    "\n",
    "# copy connector to S3 bucket \n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://user-0a60b9a8a831-bucket/kafka-connect-s3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Create custom plugin in the MSK Connect console   \n",
    "MSK console > MSK Connect section  > Custom plugins > Create custom plugin.\n",
    "Choose bucket where Confluent connector ZIP file is (s3://user-0a60b9a8a831-bucket/kafka-connect-s3/confluentinc-kafka-connect-s3-10.0.3.zip) and name this plugin 0a60b9a8a831-plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create a connector in MSK connect using custom plugin    \n",
    " \n",
    "MSK connect > Customised plugins > choose 0a60b9a8a831-plugin > Create connector >\n",
    "Connector properties\n",
    "- **Basic properties**\n",
    "    - Connector name : 0a60b9a8a831-connector\n",
    "    - Description – optional : Connecting topics to s3 bucket\n",
    "- **Apache Kafka cluster**\n",
    "    - Cluster type : MSK cluster\n",
    "    - MSK clusters : pinterest-msk-cluster\n",
    "- **Connector configuration**\n",
    "    - Configuration settings :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.class=io.confluent.connect.s3.S3SinkConnector    \n",
    "s3.region=us-east-1     \n",
    "flush.size=1    \n",
    "schema.compatibility=NONE   \n",
    "tasks.max=3     \n",
    "topics.regex=0a60b9a8a831.*     \n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat     \n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner   \n",
    "value.converter.schemas.enable=false    \n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter     \n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage     \n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter  \n",
    "s3.bucket.name=user-0a60b9a8a831-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Connector capacity**\n",
    "    - Capacity type : Provisioned\n",
    "    - MCU count per worker : 1\n",
    "    - Number of workers : 1\n",
    "- **Worker configuration**\n",
    "    -Use a customised configuration\n",
    "    - Worker configuration : confluent worker\n",
    "- **Access permision**\n",
    "    - IAM role : 0a60b9a8a831-ec2-access-role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Processing : Configuring an API in API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Build a Kafka REST proxy integration method for the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a REST API\n",
    "\n",
    "API gateway > Create API > REST API > Build > \n",
    "API name: 0a60b9a8a831\n",
    "\n",
    "Remaining settings remain as default.Then create API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a resource that allows building a PROXY integration for the API.    \n",
    "\n",
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/5.1_create_api_resource.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the created resource, Set up a HTTP ANY method    \n",
    "\n",
    "![](/home/mhash/pinterest_data_pipeline_project/Media/5.1_create_http_any_method.PNG)\n",
    "\n",
    "\n",
    "endpoint URL : http://ec2-34-207-200-90.compute-1.amazonaws.com:8082/{proxy}\n",
    "\n",
    "\n",
    "Deploy the API and note the invoke URL for a later step.    \n",
    "Invoke URL : https://vqbq2ubp7a.execute-api.us-east-1.amazonaws.com/prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Set up the Kafka REST proxy on EC2 client machine\n",
    "\n",
    "##### Install Confluent package for Kafka REST proxy on EC2 client machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz      \n",
    "tar -xvzf confluent-7.2.0.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow the REST proxy to perform IAM authentication to the MSK cluster by modifying the kafka-rest.properties file.\n",
    "Navigate to `confluent-7.2.0/etc/kafka-rest`. Inside here run the following command to modify the kafka-rest.properties file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd confluent-7.2.0/etc/kafka-rest\n",
    "nano kafka-rest.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inside the `kafka-rest.properties`. Modify the bootstrap.servers and the zookeeper.connect variables in this file, with the corresponding Boostrap server string and Plaintext Apache Zookeeper connection string, gathered back in section 3.4.     \n",
    "To surpass the IAM authentication of the MSK cluster, we will make use of the IAM MSK authentication package again, adding this at the bottom of `kafka-rest.properties`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2018 Confluent Inc.\n",
    "#\n",
    "# Licensed under the Confluent Community License (the \"License\"); you may not use\n",
    "# this file except in compliance with the License.  You may obtain a copy of the\n",
    "# License at\n",
    "#\n",
    "# http://www.confluent.io/confluent-community-license\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "#\n",
    "\n",
    "#id=kafka-rest-test-server\n",
    "#schema.registry.url=http://localhost:8081\n",
    "zookeeper.connect=z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181\n",
    "bootstrap.servers=b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098\n",
    "\n",
    "\n",
    "# Configure interceptor classes for sending consumer and producer metrics to Confluent Control Center\n",
    "# Make sure that monitoring-interceptors-<version>.jar is on the Java class path\n",
    "#consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\n",
    "#producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\n",
    "\n",
    "# Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start the REST proxy on the EC2 client machine\n",
    "To make sure messages are consumed in MSK, start the REST proxy, this also functions as a test.    \n",
    "Showing the INFO Server started and listening for requests inside EC2 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd confluent-7.2.0/bin  \n",
    "./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Sending data to the API   \n",
    "\n",
    "Modify the user_posting_emulation.py to send data to Kafka topics using API Invoke URL. Send data from the three tables to their corresponding Kafka topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opened three separate terminals, each inside the `kafka_2.12-2.8.1/bin` directory of the ec2 client, Then set up the kafka consumers one per topic for the three topics. When running user_posting_emulation.py, to send a stream of messages to the cluster, all three consumer terminals alongside the REST proxy terminal actively streamed data, with the correct data consumed by the corresponding topic. Messages also showed up in the S3 bucket, inside a folder named 'Topics'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-console-consumer.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.pin --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.geo --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.user --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 6. Batch processing : Mount AWS S3 bucket onto Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean and query batch data, we will mount and read the data from the s3 bucket into Databricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Create three different DataFrames:\n",
    "\n",
    "- df_pin for the Pinterest post data  \n",
    "- df_geo for the geolocation data     \n",
    "- df_user for the user data  \n",
    "\n",
    "This was all carried out in databricks notebook `dataframe_creation_from_s3_bucket`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 7. Clean all three dataframes and query the data on databricks using pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Batch processing: Orchestrating databricks workload using AWS MWAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Create and upload DAG to an MWAA environment  \n",
    "\n",
    "MWAA requires an S3 bucket to hold Directed Acyclic Graphs (DAGs), Python requirements and plugins. Then the MWAA airflow console can be used to run the DAGs.   \n",
    "\n",
    "I have been provided with an MWAA environment linked to an S3 bucket. However this can be created using the following steps:    \n",
    "\n",
    "##### 1) Amazon S3 > Bucket > create bucket:     \n",
    "- General configuration:  \n",
    "    - AWS region : US East (N. Virginia) us-east-1  \n",
    "    - Bucket type : General purpose \n",
    "    - Bucket name: mwaa-dags-bucket \n",
    "- Block Public Access settings for this bucket:   \n",
    "    - Block all public access   \n",
    "- Bucket Versioning:  \n",
    "    - Enable    \n",
    "     \n",
    "After creating the bucket, create a folder named `dags` inside it.  \n",
    "\n",
    "##### 2) Amazon MWAA (us-east-1) > create environment :  \n",
    "- Environment details:    \n",
    "    - Name: Databricks-Airflow-env  \n",
    "- DAG code in Amazon S3:  \n",
    "    - S3 Bucket : s3://mwaa-dags-bucket  (browse S3 and choose the previously created bucket)   \n",
    "    - DAGs folder : s3://mwaa-dags-bucket/dags (choose the dags folder created inside the S3 bucket)    \n",
    "- Then on networking page,    \n",
    "    - select create MWAA VPC    \n",
    "    - Choose the preferred Apache Airflow access mode.  \n",
    "    - Web server access : Private network   \n",
    "    - Security groups : Create new security group   \n",
    "- Environment class :     \n",
    "    - Class : mw1.small (recommended to choose the smallest environment size that is necessary to support the workload) \n",
    "    - Maximum worker count : 5  \n",
    "    - Minimum worker count : 1  \n",
    "    - Scheduler count : 2   \n",
    "\n",
    "##### 3) Create an API token in Databricks   \n",
    "In databricks, Username > User Settings > Access tokens > Generate new token > Copy the Token ID.   \n",
    "\n",
    "##### 4) Amazon MWAA > Environments > Databricks-Airflow-env > Open Airflow UI > Admin > Connections > databricks_default > Edit record:\n",
    "- Host column : <url of your Databricks account>\n",
    "- Extra column : < {\"token\": \"<API_token_id_from_previous_step>\", \"host\": \"<url_from_host_column>\"}>\n",
    "- Connection type column: Databricks\n",
    "    \n",
    "Obtaining Databricks connection type requires installalation of the corresponding Python dependencies for the MWAA environment.     \n",
    "To create and test, before uploading a requirements.txt file in the `mwaa-dags-bucket` S3 bucket, the following Github repository can be used:  https://github.com/aws/aws-mwaa-local-runner    \n",
    "After uploading requirements.txt to the S3 bucket, Amazon MWAA > environment > select environment > edit:   \n",
    "- DAG code in Amazon S3:  \n",
    "    - Requirements file : s3://mwaa-dags-bucket/requirements.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Upload DAG to MWAA environment\n",
    "\n",
    "Amazon MWAA > Environments > Databricks-Airflow-env > Airflow UI from the MWAA environment\n",
    "\n",
    "Create an Airflow DAG that will trigger a Databricks Notebook by upload the corresponding `0a60b9a8a831_dag` Python file in the `mwaa-dags-bucket/dags` S3 bucket folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpause the DAG from AWS MWAA airflow UI, and trigger the DAG. **DAG has failed** (Requires search in logs for debugging). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Stream processing: AWS Kinesis   \n",
    "\n",
    "On AWS Kinesis > Create new stream, the following 3 streams were created to link the three Pinterest tables.  \n",
    "\n",
    "- streaming-0a60b9a8a831-pin  \n",
    "- streaming-0a60b9a8a831-geo    \n",
    "- streaming-0a60b9a8a831-user     \n",
    "\n",
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/9.1_kinesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 \n",
    "\n",
    "\n",
    "Configure previously created REST API to allow it to invoke Kinesis actions. AWS account has been granted permissions to invoke Kinesis actions, did not need to create an IAM role for API to access Kinesis. However this can be done with the following steps:    \n",
    " \n",
    "Copy the ARN of the access role `0a60b9a8a831-kinesis-access-role` from the IAM console, under Roles. ARN to be used when setting up the Execution role for the integration point of all the API methods created.\n",
    "\n",
    "Enable API to be able to invoke the following actions:\n",
    "\n",
    "- List streams in Kinesis\n",
    "- Create, describe and delete streams in Kinesis\n",
    "- Add records to streams in Kinesis\n",
    "\n",
    "\n",
    "##### 9.2.1 List streams in Kinesis     \n",
    "\n",
    "\n",
    "##### 9.2.2 Create, describe and delete streams in Kinesis  \n",
    "\n",
    "\n",
    "##### 9.2.3 Add records to streams in Kinesis   \n",
    "\n",
    "\n",
    " Under {stream-name} resource create a two new child resources with the Resource Name, record and records. For both resources create a PUT method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " Resource path : /streams/stream-name/  \n",
    " Resource name : record \n",
    "\n",
    "Resource path : /streams/stream-name/   \n",
    " Resource name : records    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a `PUT` method for record and records \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the API has been redeployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Sending Data to the Kinesis streams    \n",
    "\n",
    "Create a new script `user_posting_emulation_streaming.py` to send requests to the newly created API, sending data one at a time from the three Pinterest AWS RDS tables to their corresponding Kinesis streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4 Read data from Kinesis streams into Databricks   \n",
    "\n",
    "##### 9.4.1\n",
    "Create a new notebook called `pinterest_streaming_data` in Databricks. \n",
    "1. read in credentials authentication_credentials.csv file to retrieve the Access Key and Secret Access Key.\n",
    "2. Create functions to ingest data into Kinesis Data Streams\n",
    "3. Read the data from the three streams\n",
    "4. Clean the streaming data\n",
    "5. Write each stream in a Delta Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.5 Transform Kinesis Streams in Databricks\n",
    "\n",
    "Clean the streaming data in the same way as the batch data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.6 Write streaming data into Databricks delta tables\n",
    "save each stream in a Delta Table\n",
    "The following three tables were created: \n",
    " - 0a60b9a8a831_pin_table \n",
    " - 0a60b9a8a831_geo_table \n",
    " - 0a60b9a8a831_user_table\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements: \n",
    "The cleaning for both batch and streaming data is exactly the same in both notebooks, similarly the authenticatipn credentials file is also the same, it would be nice to not have to repeat both of these sections, but unsure how to go about this due to it being a notebook - will research further. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
