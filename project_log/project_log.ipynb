{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up a  new conda environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n pinterest_conda_env\n",
    "conda activate pinterest_conda_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installed any libraries required to run the user_posting_emulation.py using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install <library_name>\n",
    "conda install -c conda-forge <library_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch processing : Configuring the EC2 Kafka client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The EC2 instance   \n",
    "AWS EC2: Cloud-hosted virtual machine\n",
    "\n",
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/ec2_instance.png)\n",
    "\n",
    "- Name: 0a60b9a8a831  \n",
    "- Instance ID : i-034925981e7bb03f3   \n",
    "- Instance type: t2.micro \n",
    "- Amazon Machine Image : Amazon linux 2023 AMI\n",
    "- Availability Zone: us-east-1a   \n",
    "- Public IPV4 DNS : ec2-34-207-200-90.compute-1.amazonaws.com \n",
    "- Public IPv4 address : 34.207.200.90 \n",
    "- Key pair name : 0a60b9a8a831-key-pair\n",
    "- Key pair type : RSA\n",
    "- Private key file format : .pem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Created .pem file locally\n",
    "EC2 > Instances > Search \"i-034925981e7bb03f3\" > Details > Key pair assigned at launch.  \n",
    "Copied the key-pair and saved inside a .pem file, and ensured that the .pem file has read-only permission for User class.   \n",
    ">chmod 400 0a60b9a8a831-key-pair.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Connect to the EC2 instance\n",
    "Followed the instructions on AWS on how to connect to EC2 through the CLI.  \n",
    "    \n",
    "> ssh -i \"0a60b9a8a831-key-pair.pem\" ec2-user@ec2-34-207-200-90.compute-1.amazonaws.com     \n",
    "    \n",
    "`ssh -i \"0a60b9a8a831-key-pair.pem\" ec2-user@ec2-34-207-200-90.compute-1.amazonaws.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Installing Kafka on the EC2 client \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been provided with access to an IAM authenticated MSK cluster.\n",
    "\n",
    "However MSK cluster can be setup manually by: \n",
    "\n",
    "- Creating the MSK cluster: \n",
    "    - MSK console > Create Cluster > Quick create    \n",
    "    - Cluster name :pinterest-msk-cluster    \n",
    "    - Cluster type : provisioned \n",
    "    - Apache Kafka version : 2.8.1   \n",
    "    - Broker type : kafka.m5.large   \n",
    "    - Amazon EBS storage per broker : 100 GiB    \n",
    "Create cluster  \n",
    "\n",
    "EC2 instance has been created, however to ensure the client machine can send data to the MSK cluster. \n",
    "VPC > Security > Security groups >  \n",
    "- Select the default security group associated with the cluster VPC (MSK console > properties tab > Networking settings section > Security groups applied) \n",
    "\n",
    "Edit inbound rules > Add rule. \n",
    "- Type column: All traffic     \n",
    "- Source column: ID of the security group of the client machine (found in EC2 console > security tab). \n",
    "- Save rules and cluster will accept all traffic from the client machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1 : First installed java and checked its version `java -version`. Then downloaded and 'unzipped' kafka version 2.12-2.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo yum install java-1.8.0\n",
    "\n",
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz   # Get the version of kafka to install\n",
    "tar -xzf kafka_2.12-2.8.1.tgz                                           # To 'unzip' or 'untar' the file \n",
    "rm kafka_2.12-2.8.1.tgz                                                 # Remove the compressed file, keeping only uncompressed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 : Installing IAM MSK authentication package on client EC2 machine.\n",
    "Entered inside the Kafka installation directory and then in the libs subdirectory. Downloaded the IAM MSK authentication package from Github (Required for a connection to MSK clusters which uses IAM authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kafka_2.12-2.8.1/libs\n",
    "\n",
    "wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.5/aws-msk-iam-auth-1.1.5-all.jar  # Download msk iam authentication file\n",
    "export CLASSPATH=/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar # Set class path environment variable for MSK authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added above classpath environment variable to bashrc file to maintain across ec2 sessions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano /home/ec2-user/.bashrc\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/home/mhash/pinterest_data_pipeline_project/project_log/nano_bashrc_20231211.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $CLASSPATH                          # To test, should return class path in configured correctly /home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Step 3 : To assume the \"0a60b9a8a831-ec2-access-role\" IAM role, which contains the necessary permissions to authenticate the MSK cluster\n",
    "    \n",
    "IAM console > Roles > 0a60b9a8a831-ec2-access-role      \n",
    "- copy ARN      \n",
    "\n",
    "Trust relationships tab > Edit trust policy > Add a principal       \n",
    "- Selected IAM roles as the Principal type , Replace ARN with the 0a60b9a8a831-ec2-access-role ARN copied from ec2-access-role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 : Configure Kafka client to use AWS IAM\n",
    "In the EC2 client, inside the kafka/bin directory `cd kafka_2.12-2.8.1/bin`, modified the client.properties file `nano client.properties` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/aws/aws-msk-iam-auth instructions on configuring a Kafka client to use AWS IAM with AWS_MSK_IAM mechanism\n",
    "\n",
    "# Sets up TLS (Transport Layer Security) for encryption (cryptographic protocol) and SASL (Simple Authentication and Security Layer) for authN (framework for authentication and data security in Internet protocols).\n",
    "security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Creating Kafka Topics\n",
    "\n",
    "Using the MSK Management Console to get cluster information\n",
    "Amazon MSK > pinterest-msk-cluster > View client information and save:  \n",
    "- Bootstrap servers Private endpoint (single-VPC)\n",
    "- Plaintext Apache Zookeeper connection string  \n",
    "\n",
    "topic names: \n",
    "- 0a60b9a8a831.pin for the Pinterest posts data\n",
    "- 0a60b9a8a831.geo for the post geolocation data\n",
    "- 0a60b9a8a831.user for the post user data\n",
    "\n",
    "In the EC2 client, entered inside the kafka folder, then inside the bin folder `cd kafka_2.12-2.8.1/bin`, to run the commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.pin\n",
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.geo\n",
    "./kafka-topics.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch processing : Connecting the MSK cluster to an S3 bucket.    \n",
    "The s3 bucket our data will be saved in is user-0a60b9a8a831-bucket (IAM role already set up to write to s3 bucket )\n",
    "\n",
    "firstly, downloaded confluent.io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i \n",
    "\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3 \n",
    "\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip \n",
    "\n",
    "# copy connector to S3 bucket \n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://user-0a60b9a8a831-bucket/kafka-connect-s3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Created custom plugin in the MSK Connect console   \n",
    "MSK console > MSK Connect section  > Custom plugins > Create custom plugin.\n",
    "Choose bucket where Confluent connector ZIP file is (s3://user-0a60b9a8a831-bucket/kafka-connect-s3/confluentinc-kafka-connect-s3-10.0.3.zip). Name the plugin 0a60b9a8a831-plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Created a connector in MSK connect using custom plugin    \n",
    " \n",
    "MSK connect > Customised plugins > choose 0a60b9a8a831-plugin > Create connector >\n",
    "Connector properties\n",
    "- **Basic properties**\n",
    "    - Connector name : 0a60b9a8a831-connector\n",
    "    - Description â€“ optional : Connecting topics to s3 bucket\n",
    "- **Apache Kafka cluster**\n",
    "    - Cluster type : MSK cluster\n",
    "    - MSK clusters : pinterest-msk-cluster\n",
    "- **Connector configuration**\n",
    "    - Configuration settings :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.class=io.confluent.connect.s3.S3SinkConnector    \n",
    "s3.region=us-east-1     \n",
    "flush.size=1    \n",
    "schema.compatibility=NONE   \n",
    "tasks.max=3     \n",
    "topics.regex=0a60b9a8a831.*     \n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat     \n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner   \n",
    "value.converter.schemas.enable=false    \n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter     \n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage     \n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter  \n",
    "s3.bucket.name=user-0a60b9a8a831-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Connector capacity**\n",
    "    - Capacity type : Provisioned\n",
    "    - MCU count per worker : 1\n",
    "    - Number of workers : 1\n",
    "- **Worker configuration**\n",
    "    -Use a customised configuration\n",
    "    - Worker configuration : confluent worker\n",
    "- **Access permision**\n",
    "    - IAM role : 0a60b9a8a831-ec2-access-role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Processing : Configuring an API in API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Built a Kafka REST proxy integration method for the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API gateway > Create API > REST API > Build > \n",
    "API name: 0a60b9a8a831\n",
    "Remaining settings remain as default.\n",
    "Create API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created resource something something \n",
    "[] video here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ANY method on proxy \n",
    "\n",
    "endpoint URL : http://ec2-34-207-200-90.compute-1.amazonaws.com:8082/{proxy}\n",
    "\n",
    "\n",
    "Deploy API  \n",
    "Invoke URL : https://vqbq2ubp7a.execute-api.us-east-1.amazonaws.com/prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Set up the Kafka REST proxy on the EC2 client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installed Confluent package for Kafka REST proxy on EC2 client machine \n",
    "\n",
    "sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz      \n",
    "\n",
    "tar -xvzf confluent-7.2.0.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow the REST proxy to perform IAM authentication to the MSK cluster by modifying the kafka-rest.properties file.\n",
    "navigate to confluent-7.2.0/etc/kafka-rest. Inside here run the following command to modify the kafka-rest.properties file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd confluent-7.2.0/etc/kafka-rest\n",
    "nano kafka-rest.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " bootstrap.servers and the zookeeper.connect variables in this file, with the corresponding Boostrap server string and Plaintext Apache Zookeeper connection string     \n",
    "To surpass the IAM authentication of the MSK cluster, we will make use of the IAM MSK authentication package again. Added to kafka-rest.properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2018 Confluent Inc.\n",
    "#\n",
    "# Licensed under the Confluent Community License (the \"License\"); you may not use\n",
    "# this file except in compliance with the License.  You may obtain a copy of the\n",
    "# License at\n",
    "#\n",
    "# http://www.confluent.io/confluent-community-license\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "#\n",
    "\n",
    "#id=kafka-rest-test-server\n",
    "#schema.registry.url=http://localhost:8081\n",
    "zookeeper.connect=z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181\n",
    "bootstrap.servers=b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098\n",
    "\n",
    "#\n",
    "# Configure interceptor classes for sending consumer and producer metrics to Confluent Control Center\n",
    "# Make sure that monitoring-interceptors-<version>.jar is on the Java class path\n",
    "#consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\n",
    "#producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\n",
    "\n",
    "# Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start the REST proxy on the EC2 client machine.\n",
    "To make sure messages are consumed in MSK, we need to start our REST proxy (also as a test)     \n",
    "Saw a INFO Server started, listening for requests... in your EC2 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd confluent-7.2.0/bin\n",
    "./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Sending data to the API   \n",
    "\n",
    "Modify the user_posting_emulation.py to send data to Kafka topics using API Invoke URL. Send data from the three tables to their corresponding Kafka topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opened three separate terminals, each inside the `kafka_2.12-2.8.1/bin` directory of the ec2 client and set up the kafka consumers for the three topics, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-console-consumer.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.pin --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.geo --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.user --from-beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
