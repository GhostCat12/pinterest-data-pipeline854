{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a  new conda environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n pinterest_conda_env\n",
    "conda activate pinterest_conda_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any libraries required to run the user_posting_emulation.py using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install <library_name>\n",
    "conda install -c conda-forge <library_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data infrastructure simulating Pinterest.\n",
    "\n",
    "The user_posting_emulation.py contains the login credentials for a RDS database, which contains three tables with data resembling data received by the Pinterest API when a POST request is made by a user uploading data to Pinterest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch processing : Configuring the EC2 Kafka client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an MSK cluster, a client is needed to communicate with this MSK cluster. In this project, an EC2 instance is used to act as the client. In my case, both MSK Cluster and EC2 instance have already been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create an MSK cluster:**\n",
    "\n",
    "Amazon MSK > Create cluster > Choose quick create\n",
    "\n",
    "> - Name cluster 'pinterest-msk-cluster       \n",
    "> - Choose Cluster type: provisioned  \n",
    "> - Apache Kafka version: 2.8.1   \n",
    "> - Choose Broker type: Kafka.m5.large    \n",
    "> - Amazon EBS storage per broker: 100 GiB    \n",
    "\n",
    "Select create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create an EC2 instance:**   \n",
    "\n",
    ">- Name: 0a60b9a8a831  \n",
    ">- Instance ID : i-034925981e7bb03f3   \n",
    ">- Instance type: t2.micro \n",
    ">- Amazon Machine Image : Amazon linux 2023 AMI\n",
    ">- Availability Zone: us-east-1a   \n",
    ">- Public IPV4 DNS : ec2-34-207-200-90.compute-1.amazonaws.com \n",
    ">- Public IPv4 address : 34.207.200.90 \n",
    ">- Key pair name : 0a60b9a8a831-key-pair\n",
    ">- Key pair type : RSA\n",
    ">- Private key file format : .pem\n",
    "\n",
    "An IAM role was also setup for the EC2 to allow Kafka access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create .pem file locally\n",
    "\n",
    "Use the key pair assigned at launched of EC2 instance to create .pem file locally, this allows secure local connection to the EC2 instance via SSH.\n",
    "\n",
    "EC2 > Instances > Search \"i-034925981e7bb03f3\" > Details > Key pair assigned at launch.\n",
    "\n",
    "Copy the key pair and save inside a .pem file, and ensure that the .pem file has read-only permission for User class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 400 0a60b9a8a831-key-pair.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Connect to the EC2 instance\n",
    "Follow the instructions on AWS on how to connect to EC2 through the CLI.  \n",
    "    \n",
    "> ssh -i \"0a60b9a8a831-key-pair.pem\" ec2-user@ec2-34-207-200-90.compute-1.amazonaws.com     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Installing Kafka on the EC2 client \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the client machine to connect and send data to the MSK cluster, it's necessary to edit the inbound rules for the security group associated with the MSK cluster.\n",
    "\n",
    "VPC > Security > Security groups > Select the default security group associated with the cluster VPC \n",
    "\n",
    "(The associated Security group can be found in MSK console > properties tab > Networking settings section > Security groups applied) \n",
    "\n",
    "Edit inbound rules > Add rule :\n",
    "> - Type column: All traffic     \n",
    "> - Source column: ID of the security group of the client machine (found in EC2 console > security tab). \n",
    "> - Save rules and cluster will accept all traffic from the client machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Install Java and Kafka on the EC2 client machine\n",
    "\n",
    "First install java and ensure it's the correct version `java -version`.     \n",
    "Then download and 'unzip' kafka version 2.12-2.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo yum install java-1.8.0\n",
    "\n",
    "# Get the version of kafka to install\n",
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz \n",
    "# To 'unzip' or 'untar' the file\n",
    "tar -xzf kafka_2.12-2.8.1.tgz  \n",
    "# Remove the compressed file, keeping only uncompressed version                                          \n",
    "rm kafka_2.12-2.8.1.tgz                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Installing IAM MSK authentication package on client EC2 machine.\n",
    "\n",
    "The [IAM MSK authentication package](https://github.com/aws/aws-msk-iam-auth) is required for verifying a client which uses IAM authentication for connecting it to MSK clusters. \n",
    "\n",
    "Inside the 'Kafka/libs' directory, download the IAM MSK authentication package from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kafka_2.12-2.8.1/libs\n",
    "# Download msk iam authentication file\n",
    "wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.5/aws-msk-iam-auth-1.1.5-all.jar  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the client classpath environment variable in `.bashrc` so the client is able to use the IAM package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open bash configuration file\n",
    "nano /home/ec2-user/.bashrc\n",
    "\n",
    "# Set class path environment variable for MSK authentication inside nano file \n",
    "export CLASSPATH=/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar #\n",
    "\n",
    "# Saves and updates the bashrc file\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/log/nano_bashrc_20231211.png)\n",
    "**(Remove image?)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If configured correctly, `echo $CLASSPATH` returns the class path '/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Authenticate MSK cluster using EC2 IAM role \n",
    "\n",
    "Before configuring the EC2 client to use AWS IAM for cluster authentication, a setup is required to ensure that the EC2 instance has the necessary IAM role and permissions to authenticate and interact with the MSK cluster securely. A trust relationship can allow the EC2 instance to assume its own IAM role (essentially saying, \"This EC2 IAM role is trusted by itself to assume its own role.\"), which contains the required permissions for MSK authentication.\n",
    "\n",
    "To assume the \"0a60b9a8a831-ec2-access-role\" EC2 IAM role, which contains the necessary permissions to authenticate the MSK cluster. First, retrieve the 0a60b9a8a831-ec2-access-role ARN.\n",
    "    \n",
    "IAM console > Roles > 0a60b9a8a831-ec2-access-role > copy ARN      \n",
    "\n",
    "Trust relationships tab > Edit trust policy > Add a principal       \n",
    "> - Selected IAM roles as the Principal type \n",
    "> - Replace ARN with the 0a60b9a8a831-ec2-access-role ARN (copied from ec2-access-role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Configure Kafka client to use AWS IAM authentication to the cluster \n",
    "\n",
    "Configure Kafka client to use AWS IAM authentication for authenticating the MSK cluster.\n",
    "In the EC2 client, inside the `kafka_2.12-2.8.1/bin` directory, usign nano, modify the `client.properties` file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "https://github.com/aws/aws-msk-iam-auth instructions on configuring a Kafka client to use \n",
    "AWS IAM with AWS_MSK_IAM mechanism\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Sets up TLS (Transport Layer Security) for encryption (cryptographic protocol) and SASL (Simple \n",
    "Authentication and Security Layer) for authN (framework for authentication and data security in Internet protocols).\n",
    "\"\"\"\n",
    "security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials. \n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "sasl.client.callback.handler.class = \\\n",
    "    software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Creating Kafka topics\n",
    "\n",
    "The bootstrap server and zookeeper string are required to create Kafka topics on the Kafka cluster using the EC2 client.  \n",
    "\n",
    "Using the MSK Management Console to get cluster information\n",
    "Amazon MSK > pinterest-msk-cluster > View client information and note:  \n",
    "- Bootstrap servers Private endpoint (single-VPC)\n",
    "- Plaintext Apache Zookeeper connection string  \n",
    "\n",
    "The three topic names being created are: \n",
    "- 0a60b9a8a831.pin for the Pinterest posts data\n",
    "- 0a60b9a8a831.geo for the post geolocation data\n",
    "- 0a60b9a8a831.user for the post user data\n",
    "\n",
    "In the EC2 client, inside the `kafka_2.12-2.8.1/bin` directory, run the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.pin\n",
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.geo\n",
    "./kafka-topics.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --command-config client.properties --create --topic 0a60b9a8a831.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch processing : Connecting the MSK cluster to an S3 bucket.    \n",
    "\n",
    "Next, configure MSK Connect to enable the MSK cluster to automatically transmit and store data to an S3 bucket, that is partitioned by topic. This is achieved by downloading the Confluent.io Amazon S3 Connector and adding it to the S3 bucket through the EC2 client. Then creating a connector in MSK connect by using the newly created custom plugin (which is designed to connect to the S3 bucket)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1:** Download confluent.io on the EC2 client and copy it to the S3 bucket in the EC2 client. \n",
    "The s3 bucket our data will be saved in is user-0a60b9a8a831-bucket (IAM role already set up to write to the S3 bucket).\n",
    "Download the confluent package : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i \n",
    "\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3 \n",
    "\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/ \\\n",
    "    versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip\n",
    "\n",
    "# copy connector to S3 bucket \n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://user-0a60b9a8a831-bucket/kafka-connect-s3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2 :** Create custom plugin in the MSK Connect console\n",
    "   \n",
    "MSK console > MSK Connect section  > Custom plugins > Create custom plugin. \n",
    "    \n",
    ">- Name this plugin 0a60b9a8a831-plugin\n",
    ">- Choose bucket where Confluent connector ZIP file is located (s3://user-0a60b9a8a831-bucket/kafka-connect-s3/confluentinc-kafka-connect-s3-10.0.3.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3:** Create a connector in MSK connect using custom plugin to connect to S3. \n",
    "\n",
    "MSK connect > Customised plugins > choose 0a60b9a8a831-plugin > Create connector > Connector properties\n",
    "> - **Basic properties**\n",
    ">    - Connector name : 0a60b9a8a831-connector\n",
    ">    - Description â€“ optional : Connecting topics to s3 bucket\n",
    "> - **Apache Kafka cluster**\n",
    ">    - Cluster type : MSK cluster\n",
    ">    - MSK clusters : pinterest-msk-cluster\n",
    "> - **Connector configuration**\n",
    ">    - Configuration settings :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector.class=io.confluent.connect.s3.S3SinkConnector    \n",
    "s3.region=us-east-1     \n",
    "flush.size=1    \n",
    "schema.compatibility=NONE   \n",
    "tasks.max=3     \n",
    "topics.regex=0a60b9a8a831.*     \n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat     \n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner   \n",
    "value.converter.schemas.enable=false    \n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter     \n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage     \n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter  \n",
    "s3.bucket.name=user-0a60b9a8a831-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Connector capacity**\n",
    ">    - Capacity type : Provisioned\n",
    ">    - MCU count per worker : 1\n",
    ">    - Number of workers : 1\n",
    ">- **Worker configuration**\n",
    ">    -Use a customised configuration\n",
    ">    - Worker configuration : confluent worker\n",
    ">- **Access permision**\n",
    ">    - IAM role : 0a60b9a8a831-ec2-access-role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Processing : Configuring an API in API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Build a Kafka REST proxy integration method for the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an API is needed to send data to the MSK cluster and in turn, the S3 bucket.\n",
    "\n",
    "##### **Step 1:** Create a REST API: \n",
    "\n",
    "API gateway > Create API > REST API > Build > \n",
    ">- API name: 0a60b9a8a831\n",
    ">- Remaining settings remain as default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Create a resource that allows building a PROXY integration for the API and setup a HTTP ANY method onto the created resource.\n",
    "\n",
    ">- Integration type: HTTP Proxy\n",
    ">- HTTP method: ANY\n",
    ">- Endpoint URL: http://ec2-34-207-200-90.compute-1.amazonaws.com:8082/{proxy} (EC2 instance as endpoint)\n",
    ">- Content handling: Passthrough (To pass all payloads to backend, apply a mapping template if specified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/log/5.1_create_api_resource.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media//log/5.1_create_http_any_method.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3:** Deploy the API and note the invoke URL so it can be used for POST requests.    \n",
    "Invoke URL : https://vqbq2ubp7a.execute-api.us-east-1.amazonaws.com/prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Set up the Kafka REST proxy on EC2 client machine\n",
    "\n",
    "Now that the API has been set up to send data to the EC2 client. Install the Confluent package to setup a REST proxy API on EC2 client which listens for requests and interacts with the kafka cluster.\n",
    "\n",
    "\n",
    "##### **Step 1:** Install Confluent package for Kafka REST proxy on EC2 client machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz      \n",
    "tar -xvzf confluent-7.2.0.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Allow the REST proxy to perform IAM authentication to the MSK cluster by modifying the kafka-rest.properties file.\n",
    "Navigate to `confluent-7.2.0/etc/kafka-rest`, and modify the `kafka-rest.properties` file.\n",
    "\n",
    "Inside the `kafka-rest.properties` file. Modify the bootstrap.servers and the zookeeper.connect variables in this file, with the corresponding Boostrap server string and Plaintext Apache Zookeeper connection string, gathered back in [Section 3.4](#3.4-Creating-Kafka-topics).\n",
    "     \n",
    "To surpass the IAM authentication of the MSK cluster, we will make use of the IAM MSK authentication package again, adding this at the bottom of `kafka-rest.properties`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2018 Confluent Inc.\n",
    "#\n",
    "# Licensed under the Confluent Community License (the \"License\"); you may not use\n",
    "# this file except in compliance with the License.  You may obtain a copy of the\n",
    "# License at\n",
    "#\n",
    "# http://www.confluent.io/confluent-community-license\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "#\n",
    "\n",
    "#id=kafka-rest-test-server\n",
    "#schema.registry.url=http://localhost:8081\n",
    "zookeeper.connect=z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster\\\n",
    "    .w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181\n",
    "bootstrap.servers=b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster\\\n",
    "    .w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098\n",
    "\n",
    "\n",
    "# Configure interceptor classes for sending consumer and producer metrics to Confluent Control Center\n",
    "# Make sure that monitoring-interceptors-<version>.jar is on the Java class path\n",
    "#consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\n",
    "#producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\n",
    "\n",
    "# Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule \\\n",
    "    required awsRoleArn=\"arn:aws:iam::584739742957:role/0a60b9a8a831-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3:** Start the REST proxy on the EC2 client machine\n",
    "To make sure messages are consumed in MSK, start the REST proxy in 'confluent-7.2.0/bin' (this also functions as a test).    \n",
    "Showing the INFO Server started and listening for requests inside EC2 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Sending data to the API   \n",
    "\n",
    "Modify the user_posting_emulation.py to send data to Kafka topics using API Invoke URL. Send data from the three tables to their corresponding Kafka topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open three separate terminals, each inside the `kafka_2.12-2.8.1/bin` directory of the ec2 client, Then set up the kafka consumers, as shown in the code below (one terminal per topic, for the three topics). \n",
    "\n",
    "Running `user_posting_emulation_batch_data.py` posts data messages to the cluster via the API gateway and the kafka REST proxy. \n",
    " \n",
    "All three consumer terminals alongside the REST proxy terminal actively streamed data, with the correct data consumed by the corresponding topic. Messages also showed up in the S3 bucket, inside a folder named 'Topics'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-console-consumer.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.pin --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.geo --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1\\\n",
    "    .amazonaws.com:9098 --consumer.config client.properties --group students --topic 0a60b9a8a831.user --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 6. Batch processing : Mount AWS S3 bucket onto Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean and query batch data, we will mount and read the data from the s3 bucket into Databricks. \n",
    "\n",
    "A notebook `pinterest_autenticate_aws` was made to create a function which reads and extracts a delta table containg the AWS authentication keys:\n",
    "1. Read the credentials delta table into a Sparks dataframe\n",
    "2. Create variables using AWS access key and secret key from the spark dataframe and return these variables in the function\n",
    "\n",
    "Another notebook was created `mount_S3_bucket` to house a mounting function to mount the S3 bucket to Databricks:\n",
    "1. Run the 'pinterest_authenticate_aws' notebook to retrieve the AWS access key and secret key\n",
    "2. Mount the S3 bucket using the AWS key credentials\n",
    "This Notebook is run once to mount the S3 bucket. \n",
    "\n",
    "Created a third notebook `pinterest_batch_data` to read the data from the s3 bucket and convert into dataframe:\n",
    "1. Run the `pinterest_authenticate_aws` notebook to retrieve the AWS access key and secret key \n",
    "2. Read all data with a .json file extension from the S3 bucket \n",
    "3. Convert the data into a spark dataframe and dynamically generate the dataframe name\n",
    "4. Create three different DataFrames:\n",
    "    - df_pin for the Pinterest post data\n",
    "    - df_geo for the geolocation data     \n",
    "    - df_user for the user data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 7. Clean all three dataframes and query the data on databricks using pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the three dataframes df_pin, df_geo, df_user and query the data on databricks using pyspark.\n",
    "\n",
    "To clean the 3 dataframes a notebook `cleaning_utils` was made, within this 3 separate functions cleaned each separate dataframe and returned a cleaned dataframe. \n",
    "\n",
    "In the `pinterest_batch_data` notebook: \n",
    "Each dataframe(df_pin, df_geo, df_user) would be updated by running each cleaning function from the `cleaning_utils` notebook. \n",
    "The data was then queried using pyspark for the following questions:\n",
    "\n",
    "1. Find the most popular Pinterest category people post to based on their country.\n",
    "2. Find the most popular category in each year between 2018 and 2022\n",
    "3. Find the user with most followers in each country\n",
    "4. What is the most popular category people post to for different age groups?\n",
    "5. What is the median follower count for users in different age groups?\n",
    "6. Find how many users have joined each year between 2015 and 2020\n",
    "7. Find the median follower count of users based on their joining year between 2015 and 2020.\n",
    "8. Find the median follower count of users based on their joining year and age group for 2015 to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Batch processing: Orchestrating databricks workload using AWS MWAA\n",
    "\n",
    "- Upload the `0a60b9a8a831_dag.py` directed acyclic graph (DAG) file to the S3 bucket `mwaa-dags-bucket/dags` associated with the MWAA environment. This allows us to run the DAG from the AWS airflow UI\n",
    "- Utilise AWS Managed Workflows for Apache Airflow (MWAA) to automate **daily** batch processing of the previously created databricks notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Create and upload DAG to an MWAA environment  \n",
    "\n",
    "MWAA requires an S3 bucket to hold Directed Acyclic Graphs (DAGs), Python requirements and plugins. Then the MWAA airflow console can be used to run the DAGs.   \n",
    "\n",
    "I have been provided with an MWAA environment linked to an S3 bucket. However this can be created using the following steps:    \n",
    "\n",
    "##### **Step 1:** Create an S3 bucket\n",
    "Amazon S3 > Bucket > create bucket:     \n",
    ">- General configuration:  \n",
    ">    - AWS region : US East (N. Virginia) us-east-1  \n",
    ">    - Bucket type : General purpose \n",
    ">    - Bucket name: mwaa-dags-bucket \n",
    ">- Block Public Access settings for this bucket:   \n",
    ">    - Block all public access   \n",
    ">- Bucket Versioning:  \n",
    ">    - Enable    \n",
    "     \n",
    "After creating the bucket, create a folder named `dags` inside it.  \n",
    "\n",
    "##### **Step 2:** Creating the MWAA environment\n",
    "Amazon MWAA (us-east-1) > create environment :  \n",
    ">- Environment details:    \n",
    ">    - Name: Databricks-Airflow-env  \n",
    ">- DAG code in Amazon S3:  \n",
    ">    - S3 Bucket : s3://mwaa-dags-bucket  (browse S3 and choose the previously created bucket)   \n",
    ">    - DAGs folder : s3://mwaa-dags-bucket/dags (choose the dags folder created inside the S3 bucket)    \n",
    ">- Then on networking page,    \n",
    ">    - select create MWAA VPC    \n",
    ">    - Choose the preferred Apache Airflow access mode.  \n",
    ">    - Web server access : Private network   \n",
    ">    - Security groups : Create new security group   \n",
    ">- Environment class :     \n",
    ">    - Class : mw1.small (recommended to choose the smallest environment size that is necessary to support the workload) \n",
    ">    - Maximum worker count : 5  \n",
    ">    - Minimum worker count : 1  \n",
    ">    - Scheduler count : 2   \n",
    "\n",
    "##### **Step 3:** Create an API token in Databricks   \n",
    "The API token functions as a connection between Databricks and the MWAA environment. \n",
    "Username > User Settings > Access tokens > Generate new token > Copy the Token ID.   \n",
    "\n",
    "##### **Step 4:** Connect MWAA to Databricks\n",
    "Using the Databricks API token, set up the connection between MWAA and Databricks.\n",
    "\n",
    "Amazon MWAA > Environments > Databricks-Airflow-env > Open Airflow UI > Admin > Connections > databricks_default > Edit record:\n",
    ">- Host column : \\<url of your Databricks account\\>\n",
    ">- Extra column : {\"token\": \"\\<API_token_id_from_previous_step\\>\", \"host\": \"\\<url_from_host_column\\>\"}\n",
    ">- Connection type column: Databricks\n",
    "    \n",
    "##### **Step 5:** Obtaining Databricks connection type\n",
    "Obtaining Databricks connection type requires installalation of the corresponding Python dependencies for the MWAA environment This is included in the `requirements.txt` file.     \n",
    "Before uploading a `requirements.txt` file in the `mwaa-dags-bucket` S3 bucket, the following [Github repository](https://github.com/aws/aws-mwaa-local-runner) can be used to create and test the environment.\n",
    "\n",
    "After uploading requirements.txt to the S3 bucket, select the `requirements.txt` environment in amazon MWAA.\n",
    " Amazon MWAA > environment > select environment > edit:   \n",
    ">- DAG code in Amazon S3:  \n",
    ">    - Requirements file : s3://mwaa-dags-bucket/requirements.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Upload DAG to MWAA environment and trigger the DAG\n",
    " \n",
    "Create an Airflow DAG that will trigger a Databricks Notebook by upload the corresponding `0a60b9a8a831_dag` Python file in the `mwaa-dags-bucket/dags` S3 bucket folder (associated with the MWAA environment). \n",
    "\n",
    "To manually trigger the DAG: \n",
    "Amazon MWAA > Environments > Databricks-Airflow-env > Airflow UI from the MWAA environment.\n",
    "Unpause the DAG from AWS MWAA airflow UI, and trigger the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Stream processing: AWS Kinesis   \n",
    "\n",
    "To stream data from the AWS RDS to Databricks Delta tables, a new script was created to send data to an API and creating a new notebook to stream to Delta tables.   \n",
    "\n",
    "#### 9.1 Creating streams on AWS Kinesis \n",
    "First, the following 3 streams were created to link the three Pinterest tables:     \n",
    " - streaming-0a60b9a8a831-pin  \n",
    " - streaming-0a60b9a8a831-geo    \n",
    " - streaming-0a60b9a8a831-user  \n",
    "\n",
    "On AWS Kinesis > Create new stream\n",
    "    \n",
    "\n",
    "![](media/log/9.1_kinesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Modifyign the REST API\n",
    "\n",
    "Configure the previously created REST API to allow it to invoke Kinesis actions. \n",
    "AWS account has been granted permissions to invoke Kinesis actions, there was no need to create an IAM role for API to access Kinesis. However this can be done with the following steps:    \n",
    " \n",
    "Copy the ARN of the access role `0a60b9a8a831-kinesis-access-role` from the IAM console, under Roles. ARN to be used when setting up the Execution role for the integration point of all the API methods created.\n",
    "\n",
    "An overview of the REST API:\n",
    "\n",
    "![](media/log/9.2_REST_API.PNG)\n",
    "\n",
    "\n",
    "##### **Step 1:** List streams in Kinesis     \n",
    "\n",
    "To begin building the integration, navigate to the previously created API in AWS API Gateway. And select Create resource\n",
    "API Gateway > APIs > previously created API > Create resource:\n",
    "\n",
    "![](media/log/9.2.1_kinesis_api_gateway_streams.PNG)\n",
    "\n",
    "Under this newly created streams resource, create a GET method with the following settings: \n",
    "> Integration type: AWS Service     \n",
    "> AWS Region: us-east-1     \n",
    "> AWS Service: Kinesis  \n",
    "> HTTP method: POST (to invoke Kinesis's ListStreams action)    \n",
    "> Action Type: Use action name     \n",
    "> Action name: ListStreams  \n",
    "> Execution role: ARN of your Kinesis Access Role (created in the previous section)\n",
    "\n",
    "![](media/log/9.2.2_kinesis_api_gateway_get_method.PNG)\n",
    "\n",
    "Once the GET method has been created, select the Integration request panel, and click on the Edit and modify the header parameteres and mapping template. \n",
    "\n",
    "\n",
    "> URL Requests header parameters :  \n",
    ">- Name: Content-Type    \n",
    ">- Mapped from: 'application/x-amz-json-1.1'\n",
    ">\n",
    "> Mapping templates: \n",
    ">- Content type: application/json\n",
    "> - Body:  {}\n",
    "\n",
    "\n",
    "![](media/log/9.2.3_kinesis_api_gateway_integration_request_header_mapping.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Create, describe and delete streams in Kinesis  \n",
    "\n",
    "Under the streams resource create a new child resource with the Resource name {stream-name}\n",
    "\n",
    "**The settings to create a POST method:**   \n",
    "\n",
    "On the create method settings:\n",
    ">- Integration Type: AWS Service    \n",
    ">- AWS Region: us-east-1    \n",
    ">- AWS Service: Kinesis     \n",
    ">- HTTP method: POST    \n",
    ">- Action: CreateStream     \n",
    ">- Execution role: ARN of IAM role created  \n",
    "\n",
    "![](media/log/9.2.4_kinesis_api_stream_child_post_method.PNG)\n",
    "\n",
    "**In 'Integration Request' under 'Mapping Templates', add new mapping template:**\n",
    ">- Content Type: 'application/json'\n",
    ">- Mapping Template Body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"ShardCount\": #if($input.path('$.ShardCount') == '') 5 #else $input.path('$.ShardCount') #end,\n",
    "    \"StreamName\": \"$input.params('stream-name')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/log/9.2.5_kinesis_api_put_integration_request_header_mapping.PNG)\n",
    "\n",
    "**For the other methods, the same settings were used except for the action name while creating the method and mapping template for the integration request settings:**\n",
    "\n",
    "**The settings to create the GET method:** \n",
    ">- Action name: 'DescribeStream'    \n",
    ">- Mapping Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"StreamName\": \"$input.params('stream-name')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The settings to create the DELETE method:**\n",
    "\n",
    ">- Action: 'DeleteStream'\n",
    ">- Mapping Template Body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"StreamName\": \"$input.params('stream-name')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3:** Add records to streams in Kinesis   \n",
    "\n",
    "Under {stream-name} resource create a two new child resources with the Resource Name: record and records. THe settings for creating the child resources were as follows: \n",
    "\n",
    ">- Resource path : /streams/stream-name/  \n",
    ">- Resource name : record \n",
    "\n",
    ">- Resource path : /streams/stream-name/   \n",
    ">- Resource name : records  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both resources create a PUT method.\n",
    "\n",
    "**The settings to create the PUT method for record:**   \n",
    ">- Action name: 'PutRecord'    \n",
    ">- Mapping Template:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"StreamName\": \"$input.params('stream-name')\",\n",
    "    \"Data\": \"$util.base64Encode($input.json('$.Data'))\",\n",
    "    \"PartitionKey\": \"$input.path('$.PartitionKey')\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The settings to create the PUT method for records:**  \n",
    ">- Action name: 'PutRecords'  \n",
    ">- Mapping Template: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"StreamName\": \"$input.params('stream-name')\",\n",
    "    \"Records\": [\n",
    "       #foreach($elem in $input.path('$.records'))\n",
    "          {\n",
    "            \"Data\": \"$util.base64Encode($elem.data)\",\n",
    "            \"PartitionKey\": \"$elem.partition-key\"\n",
    "          }#if($foreach.hasNext),#end\n",
    "        #end\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the API has been redeployed, and the invoke URL has been noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Sending Data to the Kinesis streams    \n",
    "\n",
    "The previously created script `user_posting_emulation.py` was modified to create an api_send_to_kinesis function. Create a new script `user_posting_emulation_streaming.py` to send requests to the newly created API, sending data one at a time from the three Pinterest AWS RDS tables to their corresponding Kinesis streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Read data from Kinesis streams into Databricks   \n",
    "\n",
    "\n",
    "Create a new notebook called `pinterest_streaming_data` in Databricks (Details can be found inside the notebook).\n",
    "\n",
    "1. To read the credentials import the `pinterest_authenticate_aws` notebook.\n",
    "2. Create functions to ingest data into Kinesis Data Streams\n",
    "3. Read the data from the three streams\n",
    "\n",
    "### 9.5 Transform Kinesis Streams in Databricks\n",
    "\n",
    "Clean the streaming data utilising the `cleaning_utils` notebook functions inside the `pinterest_streaming_data` notebook.\n",
    "\n",
    "### 9.6 Write streaming data into Databricks delta tables\n",
    "Created a new function `store_as_delta` in the notebook `pinterest_streaming_data` to save each stream in a Delta Table.\n",
    "\n",
    "The following three tables were created: \n",
    " - 0a60b9a8a831_pin_table \n",
    " - 0a60b9a8a831_geo_table \n",
    " - 0a60b9a8a831_user_table\n",
    "\n",
    "The delta tables could be viewed under Catalogs > default >  \\<name of delta table\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvements:**\n",
    "The cleaning for both batch and streaming data is exactly the same in both notebooks, similarly the authenticatipn credentials file is also the same, it would be nice to not have to repeat both of these sections, but unsure how to go about this due to it being a notebook - will research further. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
